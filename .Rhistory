ud_model <- udpipe_load_model(ud_model_file$file_model)
# Load your HTML file and extract paragraph texts
apple <- read_html("C:/Users/prba6/Downloads/aapl_2023-08-03.html") %>%
html_nodes('p') %>%
html_text()
# Annotate and POS tag using udpipe
annotated_apple <- udpipe_annotate(ud_model, x = str_c(apple, collapse = ' '))
df <- as.data.frame(annotated_apple)
# Filter for nouns and adjectives
filtered_df <- df %>%
filter(upos %in% c("NOUN", "ADJ")) %>%
group_by(sentence_id) %>%
summarise(filtered_text = str_c(lemma, collapse = ' '))
# Unnest to get individual sentences
sentences <- filtered_df %>%
unnest_tokens(sentence, filtered_text, token = "sentences")
# Use the SIMILARITY_SORT function from textTinyR to rank sentences
similarity_matrix <- SIMILARITY_SORT(matrix = as.matrix(sentences$sentence), method = 'cosine', sort_method = 'degree')
# Load the required libraries
library(tidyverse)
library(rvest)
library(textTinyR)
library(udpipe)
# Download and load the udpipe model (only need to download once)
ud_model_file <- udpipe_download_model(language = "english", model_dir = tempdir(), overwrite = FALSE)
ud_model <- udpipe_load_model(ud_model_file$file_model)
# Load your HTML file and extract paragraph texts
apple <- read_html("C:/Users/prba6/Downloads/aapl_2023-08-03.html") %>%
html_nodes('p') %>%
html_text()
# Annotate and POS tag using udpipe
annotated_apple <- udpipe_annotate(ud_model, x = str_c(apple, collapse = ' '))
df <- as.data.frame(annotated_apple)
# Filter for nouns and adjectives
filtered_df <- df %>%
filter(upos %in% c("NOUN", "ADJ")) %>%
group_by(sentence_id) %>%
summarise(filtered_text = str_c(lemma, collapse = ' '))
# Unnest to get individual sentences
sentences <- filtered_df %>%
unnest_tokens(sentence, filtered_text, token = "sentences")
# Use the SIMILARITY_SORT function from textTinyR to rank sentences
similarity_matrix <- SIMILARITY_SORT(matrix = as.matrix(sentences$sentence), method = 'cosine', sort_method = 'degree')
# Load the required libraries
library(tidyverse)
library(textTinyR)
library(udpipe)
# Download and load the udpipe model (only need to download once)
ud_model_file <- udpipe_download_model(language = "english", model_dir = tempdir(), overwrite = FALSE)
ud_model <- udpipe_load_model(ud_model_file$file_model)
# Annotate and POS tag using udpipe
annotated_data <- udpipe_annotate(ud_model, x = str_c(article_sentences$sentence, collapse = ' '))
# Load the required libraries
library(tidyverse)
library(tidytext)
library(rvest)
library(textTinyR)
library(udpipe)
# Load your HTML file and extract paragraph texts
apple <- read_html("C:/Users/prba6/Downloads/aapl_2023-08-03.html") %>%
html_nodes('p') %>%
html_text()
# Create the article_sentences tibble
article_sentences <- tibble(text = apple) %>%
tidytext::unnest_tokens(sentence, text, token = "sentences") %>%
dplyr::mutate(sentence_id = row_number()) %>%
dplyr::select(sentence_id, sentence)
# Download and load the udpipe model (only need to download once)
ud_model_file <- udpipe_download_model(language = "english", model_dir = tempdir(), overwrite = FALSE)
ud_model <- udpipe_load_model(ud_model_file$file_model)
# Annotate and POS tag using udpipe
annotated_data <- udpipe_annotate(ud_model, x = str_c(article_sentences$sentence, collapse = ' '))
df <- as.data.frame(annotated_data)
# Filter for nouns and adjectives
filtered_df <- df %>%
filter(upos %in% c("NOUN", "ADJ")) %>%
group_by(sentence_id) %>%
summarise(filtered_text = str_c(lemma, collapse = ' '))
# Unnest to get individual sentences
sentences <- filtered_df %>%
unnest_tokens(sentence, filtered_text, token = "sentences")
# Calculate Jaccard similarities
similarity_matrix <- textTinyR::JACCARD_DICE(as.character(sentences$sentence), method = 'jaccard', threads = 1)
# Load the required libraries
library(tidyverse)
library(tidytext)
library(rvest)
library(textTinyR)
library(udpipe)
# Load your HTML file and extract paragraph texts
apple <- read_html("C:/Users/prba6/Downloads/aapl_2023-08-03.html") %>%
html_nodes('p') %>%
html_text()
# Create the article_sentences tibble
article_sentences <- tibble(text = apple) %>%
tidytext::unnest_tokens(sentence, text, token = "sentences") %>%
dplyr::mutate(sentence_id = row_number()) %>%
dplyr::select(sentence_id, sentence)
# Download and load the udpipe model (only need to download once if you already have the model)
ud_model_file <- udpipe_download_model(language = "english", model_dir = tempdir(), overwrite = FALSE)
ud_model <- udpipe_load_model(ud_model_file$file_model)
# Annotate and POS tag using udpipe
annotated_data <- udpipe_annotate(ud_model, x = article_sentences$sentence)
df <- as.data.frame(annotated_data)
# Filter for nouns and adjectives
filtered_df <- df %>%
filter(upos %in% c("NOUN", "ADJ")) %>%
group_by(doc_id) %>%
summarise(filtered_text = str_c(lemma, collapse = ' '))
# To prepare the data for JACCARD_DICE, we need to transform our sentences into lists of terms.
sentence_terms_list <- str_split(filtered_df$filtered_text, " ")
# Calculate Jaccard similarities
similarity_matrix <- textTinyR::JACCARD_DICE(sentence_terms_list, sentence_terms_list, method = 'jaccard', threads = 1)
# Rank sentences based on mean similarity to other sentences
sentence_rank <- rowMeans(similarity_matrix$similarity)
# Load required libraries
library(tidyverse)
library(tidytext)
library(rvest)
library(udpipe)
library(textTinyR)
# 1. Load and preprocess the HTML content
apple <- read_html("C:/Users/prba6/Downloads/aapl_2023-08-03.html") %>%
html_nodes('p') %>%
html_text()
# 2. Tokenize the content into sentences
article_sentences <- tibble(text = apple) %>%
tidytext::unnest_tokens(sentence, text, token = "sentences") %>%
dplyr::mutate(sentence_id = row_number()) %>%
dplyr::select(sentence_id, sentence)
# 3. Use the `udpipe` package for part-of-speech tagging and lemmatization
# Make sure you have the English model downloaded (uncomment the next line if you haven't done it yet)
# udpipe_download_model(model = "english", model_dir = "path_to_save_model")
ud_model <- udpipe_load_model("path_to_saved_model/english-ud-2.5-191206.udpipe")
# Load required libraries
library(tidyverse)
library(tidytext)
library(rvest)
library(udpipe)
library(textTinyR)
# 1. Load and preprocess the HTML content
apple <- read_html("C:/Users/prba6/Downloads/aapl_2023-08-03.html") %>%
html_nodes('p') %>%
html_text()
# 2. Tokenize the content into sentences
article_sentences <- tibble(text = apple) %>%
tidytext::unnest_tokens(sentence, text, token = "sentences") %>%
dplyr::mutate(sentence_id = row_number()) %>%
dplyr::select(sentence_id, sentence)
# 3. Use the `udpipe` package for part-of-speech tagging and lemmatization
# Make sure you have the English model downloaded (uncomment the next line if you haven't done it yet)
# udpipe_download_model(model = "english", model_dir = "path_to_save_model")
ud_model <- udpipe_load_model("path_to_saved_model/english-ud-2.5-191206.udpipe")
# Load required libraries
library(tidyverse)
library(tidytext)
library(rvest)
library(udpipe)
library(textTinyR)
# 1. Load and preprocess the HTML content
apple <- read_html("C:/Users/prba6/Downloads/aapl_2023-08-03.html") %>%
html_nodes('p') %>%
html_text()
# 2. Tokenize the content into sentences
article_sentences <- tibble(text = apple) %>%
tidytext::unnest_tokens(sentence, text, token = "sentences") %>%
dplyr::mutate(sentence_id = row_number()) %>%
dplyr::select(sentence_id, sentence)
# 3. Use the `udpipe` package for part-of-speech tagging and lemmatization
# Use the English pre-trained model
ud_model <- udpipe::udpipe_download_model(language = "english", model_dir = tempdir(), overwrite = FALSE)
ud_model <- udpipe::udpipe_load_model(ud_model$file_model)
annotated_data <- udpipe_annotate(ud_model, x = str_c(article_sentences$sentence, collapse = ' '))
df <- as.data.frame(annotated_data)
# 4. Filter out only nouns and adjectives
filtered_df <- df %>%
filter(upos %in% c("NOUN", "ADJ")) %>%
group_by(sentence_id) %>%
summarise(filtered_text = str_c(lemma, collapse = ' '))
# 5. Calculate the Jaccard similarity between sentences
sentence_terms_list <- str_split(filtered_df$filtered_text, " ")
similarity_matrix <- textTinyR::JACCARD_DICE(sentence_terms_list, sentence_terms_list, method = 'jaccard', threads = 1)
# 6. Rank the sentences and extract the top ones
sentence_rank <- rowMeans(matrix(unlist(similarity_matrix$similarity), nrow=length(sentence_terms_list), byrow=TRUE))
# Install and load necessary libraries
install.packages(c("tidyverse", "rvest", "tidytext", "textrank"))
library(tidyverse)
library(rvest)
library(tidytext)
library(textrank)
install.packages(c("tidyverse", "rvest", "tidytext", "textrank"))
# Print the top-ranked sentences
print(ranked_sentences)
# Install and load necessary libraries
install.packages(c("tidyverse", "rvest", "tidytext", "textrank"))
library(tidyverse)
library(rvest)
library(tidytext)
library(textrank)
# 1. Load and preprocess the HTML content
apple <- read_html("C:/Users/prba6/Downloads/aapl_2023-08-03.html") %>%
html_nodes('p') %>%
html_text() %>%
as_tibble() %>%
rename(paragraph = value)
# 2. Tokenize the text into sentences
apple_sentences <- apple %>%
unnest_tokens(sentence, paragraph, token = "sentences") %>%
mutate(sentence_id = row_number()) %>%
select(sentence_id, sentence)
# 3. Implementing TextRank to rank sentences
ranked_sentences <- textrank_sentence(data = apple_sentences,
document_id = "sentence_id",
sentence = "sentence",
n = 10, # Number of sentences for the summary
verbose = FALSE)
install.packages(c("tidyverse", "rvest", "tidytext", "textrank"))
# Install and load necessary libraries
install.packages(c("tidyverse", "rvest", "tidytext", "textrank"))
library(tidyverse)
library(rvest)
library(tidytext)
library(textrank)
# 1. Load and preprocess the HTML content
apple <- read_html("C:/Users/prba6/Downloads/aapl_2023-08-03.html") %>%
html_nodes('p') %>%
html_text() %>%
as_tibble() %>%
rename(paragraph = value)
# 2. Tokenize the text into sentences
apple_sentences <- apple %>%
unnest_tokens(sentence, paragraph, token = "sentences") %>%
mutate(sentence_id = row_number()) %>%
select(sentence_id, sentence)
# 3. Implementing TextRank to rank sentences
ranked_sentences <- textrank_sentence(data = apple_sentences,
document_id = "sentence_id",
sentence = "sentence",
n = 10, # Number of sentences for the summary
verbose = FALSE)
# Read and preprocess the HTML content
apple <- read_html("C:/Users/prba6/Downloads/aapl_2023-08-03.html") %>%
html_nodes('p') %>%
html_text()
# Tokenize the paragraphs into sentences
apple_sentences <- tibble(text = apple) %>%
unnest_tokens(sentence, text, token = "sentences") %>%
mutate(sentence_id = row_number()) %>%
select(sentence_id, sentence)
# Now, we'll prepare the data for JACCARD_DICE. We will transform our sentences into lists of terms.
sentence_terms_list <- str_split(apple_sentences$sentence, " ")
# Compute Jaccard similarity matrix
similarity_matrix <- textTinyR::JACCARD_DICE(sentence_terms_list, sentence_terms_list, method = 'jaccard', threads = 1)
# Using the similarity matrix, rank the sentences
sentence_rank <- rank(-rowMeans(as.matrix(similarity_matrix$SIMILARITY)))
# Load libraries
library(tidyverse)
library(rvest)
library(tidytext)
library(textrank)
# 1. Load and preprocess the HTML content
apple <- read_html("C:/Users/prba6/Downloads/aapl_2023-08-03.html") %>%
html_nodes('p') %>%
html_text()
# 2. Tokenize the paragraphs into sentences
apple_sentences <- tibble(text = apple) %>%
unnest_tokens(sentence, text, token = "sentences") %>%
mutate(sentence_id = row_number()) %>%
select(sentence_id, sentence)
# 3. Extract ranked sentences using textrank
ranked_sentences <- textrank_sentences(data = apple_sentences, document_id = "sentence_id", n = 10)
# Load libraries
library(tidyverse)
library(rvest)
library(tidytext)
library(textrank)
# 1. Load and preprocess the HTML content
apple <- read_html("C:/Users/prba6/Downloads/aapl_2023-08-03.html") %>%
html_nodes('p') %>%
html_text()
# 2. Tokenize the paragraphs into sentences
apple_sentences <- tibble(text = apple) %>%
unnest_tokens(sentence, text, token = "sentences") %>%
mutate(sentence_id = row_number()) %>%
select(sentence_id, sentence)
# 3. Extract ranked sentences using textrank
ranked_sentences <- textrank_sentences(data = apple_sentences, document_id = "sentence_id", n = 10)
library(tidyverse)
library(rvest)
library(tidytext)
library(textrank)
# 1. Load and preprocess the HTML content
apple <- read_html("C:/Users/prba6/Downloads/aapl_2023-08-03.html") %>%
html_nodes('p') %>%
html_text()
# 2. Tokenize the paragraphs into sentences
apple_sentences <- tibble(text = apple) %>%
unnest_tokens(sentence, text, token = "sentences") %>%
mutate(sentence_id = row_number()) %>%
select(sentence_id, sentence)
# 3. Extract ranked sentences using textrank
ranked_sentences <- textrank_sentences(data = apple_sentences, document_id = "sentence_id", n = 10)
# Load libraries
library(tidyverse)
library(rvest)
library(tidytext)
library(textrank)
# 1. Load and preprocess the HTML content
apple <- read_html("C:/Users/prba6/Downloads/aapl_2023-08-03.html") %>%
html_nodes('p') %>%
html_text()
# 2. Tokenize the paragraphs into sentences
apple_sentences <- tibble(text = apple) %>%
unnest_tokens(sentence, text, token = "sentences") %>%
mutate(sentence_id = row_number()) %>%
select(sentence_id, sentence)
# 3. Extract ranked sentences using textrank
ranked_sentences <- textrank_sentences(
data = apple_sentences,
document_id = "sentence_id",
terminology = NULL,  # Assuming you don't have a terminology input for now
textrank_dist = textrank_jaccard,
textrank_candidates = textrank_candidates_all(apple_sentences$sentence_id),
max = 10,
options_pagerank = list(directed = FALSE)
# ... (Additional arguments if necessary)
)
# Inspect the column names
print(colnames(terminology))
library(stringr)
library(tidyverse)
# Step 1: Read the entire data file into memory using the readLines()-function.
url <- "http://www.sao.ru/lv/lvgdb/article/suites_dw_Table1.txt"
lines <- readLines(url, warn = FALSE)
# Load necessary libraries
library(ggplot2)
# Assuming 'data' is a dataframe containing your columns: Volume, Supply, Demand, Supply_linear, Demand_linear
data <- data.frame(
Volume = c(0, 71, 190, 228, 241, 317, 378, 479, 682, 820, 918, 1296, 1304, 1360, 1368, 1403, 1457, 1521, 1547, 1580, 1679),
Supply = c(0.1, 0.1, 4, 7.5, 11.4, 14.7, 15.5, 17.7, 18.8, 21.5, 22.9, 28.5, 33.6, 37.2, 40.3, 41.4, 43.7, 46.2, 47.5, 48.5, 51.9),
Demand = c(76.6, 76.6, 49.2, 41.6, 35.3, 33, 32.5, 31.1, 29.7, 29.1, 29, 26.5, 25, 23.7, 16.5, 16.5, 14.9, 12.3, 6.9, 1.3, 0.3),
Supply_linear = c(0.1, 0.1, 4, 7.5, 11.4, 14.7, 15.5, 17.7, 18.8, 21.5, 22.9, 28.5, 33.6, 37.2, 40.3, 41.4, 43.7, 46.2, 47.5, 48.5, 51.9),
Demand_linear = c(76.6, 76.6, 49.2, 41.6, 35.3, 33, 32.5, 31.1, 29.7, 29.1, 29, 26.5, 25, 23.7, 16.5, 16.5, 14.9, 12.3, 6.9, 1.3, 0.3)
)
# Function to find the intersection of two lines
find_intersection <- function(x1, y1, x2, y2) {
# Creating a grid to search for the intersection
x_values <- seq(min(x1, x2), max(x1, x2), length.out = 1000)
y_values_supply <- approx(x1, y1, x_values)$y
y_values_demand <- approx(x2, y2, x_values)$y
# Finding the intersection
idx <- which(abs(y_values_supply - y_values_demand) == min(abs(y_values_supply - y_values_demand)))
return(c(x = x_values[idx], y = y_values_supply[idx]))
}
# Find the intersection of the linearized supply and demand curves
intersection <- find_intersection(data$Volume, data$Supply_linear, data$Volume, data$Demand_linear)
# Plotting the data
ggplot(data, aes(x = Volume)) +
geom_step(aes(y = Supply), direction = 'hv', color = 'blue') +
geom_step(aes(y = Demand), direction = 'hv', color = 'red') +
geom_line(aes(y = Supply_linear), linetype = "dashed", color = 'blue') +
geom_line(aes(y = Demand_linear), linetype = "dashed", color = 'red') +
geom_point(aes(x = intersection$x, y = intersection$y), color = 'green', size = 4) +
labs(title = "Supply and Demand Curves", x = "Volume", y = "Price") +
theme_minimal()
# Function to find the intersection of two lines
find_intersection <- function(x1, y1, x2, y2) {
# Creating a grid to search for the intersection
x_values <- seq(min(x1, x2), max(x1, x2), length.out = 1000)
y_values_supply <- approx(x1, y1, x_values)$y
y_values_demand <- approx(x2, y2, x_values)$y
# Finding the intersection
idx <- which(abs(y_values_supply - y_values_demand) == min(abs(y_values_supply - y_values_demand)))
return(list(x = x_values[idx], y = y_values_supply[idx]))
}
# Find the intersection of the linearized supply and demand curves
intersection <- find_intersection(data$Volume, data$Supply_linear, data$Volume, data$Demand_linear)
# Plotting the data
ggplot(data, aes(x = Volume)) +
geom_step(aes(y = Supply), direction = 'hv', color = 'blue') +
geom_step(aes(y = Demand), direction = 'hv', color = 'red') +
geom_line(aes(y = Supply_linear), linetype = "dashed", color = 'blue') +
geom_line(aes(y = Demand_linear), linetype = "dashed", color = 'red') +
geom_point(aes(x = intersection$x, y = intersection$y), color = 'green', size = 4) +
labs(title = "Supply and Demand Curves", x = "Volume", y = "Price") +
theme_minimal()
# Function to find the intersection of two lines
find_intersection <- function(x1, y1, x2, y2) {
# Creating a grid to search for the intersection
x_values <- seq(min(x1, x2), max(x1, x2), length.out = 1000)
y_values_supply <- approx(x1, y1, x_values)$y
y_values_demand <- approx(x2, y2, x_values)$y
# Finding the intersection
idx <- which(abs(y_values_supply - y_values_demand) == min(abs(y_values_supply - y_values_demand)))
return(list(x = x_values[idx], y = y_values_supply[idx]))
}
# Find the intersection of the linearized supply and demand curves
intersection <- find_intersection(data$Volume, data$Supply_linear, data$Volume, data$Demand_linear)
# Print the intersection point
print(intersection)
# Continue with the plotting (if needed)
# ... rest of the plotting code
# Function to find the intersection of two lines
find_intersection <- function(x1, y1, x2, y2) {
# Creating a grid to search for the intersection
x_values <- seq(min(x1, x2), max(x1, x2), length.out = 1000)
y_values_supply <- approx(x1, y1, x_values)$y
y_values_demand <- approx(x2, y2, x_values)$y
# Finding the intersection
idx <- which(abs(y_values_supply - y_values_demand) == min(abs(y_values_supply - y_values_demand)))
return(list(x = x_values[idx], y = y_values_supply[idx]))
}
# Find the intersection of the linearized supply and demand curves
intersection <- find_intersection(data$Volume, data$Supply_linear, data$Volume, data$Demand_linear)
# Plotting the data
ggplot(data, aes(x = Volume)) +
geom_step(aes(y = Supply), direction = 'hv', color = 'blue') +
geom_step(aes(y = Demand), direction = 'hv', color = 'red') +
geom_line(aes(y = Supply_linear), linetype = "dashed", color = 'blue') +
geom_line(aes(y = Demand_linear), linetype = "dashed", color = 'red') +
geom_point(aes(x = intersection$x, y = intersection$y), color = 'green', size = 4) +
labs(title = "Supply and Demand Curves", x = "Volume", y = "Price") +
theme_minimal()
# Function to find the intersection of two lines
find_intersection <- function(x1, y1, x2, y2) {
# Creating a grid to search for the intersection
x_values <- seq(min(x1, x2), max(x1, x2), length.out = 1000)
y_values_supply <- approx(x1, y1, x_values)$y
y_values_demand <- approx(x2, y2, x_values)$y
# Finding the intersection
idx <- which(abs(y_values_supply - y_values_demand) == min(abs(y_values_supply - y_values_demand)))
return(list(x = x_values[idx], y = y_values_supply[idx]))
}
# Find the intersection of the linearized supply and demand curves
intersection <- find_intersection(data$Volume, data$Supply_linear, data$Volume, data$Demand_linear)
# Print the intersection point
print(intersection)
df <-
tibble(
a = rnorm(10),
b = rnorm(10),
c = rnorm(10),
d = rnorm(10),
e = rnorm(10)
)
# Load function for posting GQL-queries and retrieving data:
source("functions/GQL_function.r")
# Load function for posting GQL-queries and retrieving data:
source("functions/GQL_function.r")
setwd("C:/Users/prba6/iterations-promotosh")
# Load function for posting GQL-queries and retrieving data:
source("functions/GQL_function.r")
configs <-
read_yaml("vegvesen_configs.yml")
library(httr)
library(jsonlite)
library(ggplot2)
library(DescTools)
library(tidyverse)
library(magrittr)
library(rlang)
library(lubridate)
library(anytime)
library(readr)
library(yaml)
# Load function for posting GQL-queries and retrieving data:
source("functions/GQL_function.r")
configs <-
read_yaml("vegvesen_configs.yml")
gql_metadata_qry <- read_file("gql-queries/station_metadata.gql")
stations_metadata <-
GQL(
query=gql_metadata_qry,
.url = configs$vegvesen_url
)
source("functions/data_transformations.r")
transform_metadata_to_df <- function(metadata){
library(dplyr)
library(lubridate)
}
df <- metadata %>%
mutate(lastestData = as_datetime(latestData, tz="UTC")) %>%
select(id,name, lastestData, lat, lon)
source("functions/data_transformations.r")
transform_metadata_to_df <- function(metadata){
library(dplyr)
library(lubridate)
}
df <- metadata %>%
mutate(lastestData = as_datetime(latestData, tz="UTC")) %>%
select(id,name, lastestData, lat, lon)
transform_metadata_to_df <- function(stations_metadata) {
library(dplyr)
library(lubridate)
df <- stations_metadata %>%
mutate(latestData = as_datetime(latestData, tz = "UTC")) %>%
select(id, name, latestData, lat, lon)
return(df)
}
source("functions/data_transformations.r")
source("C:\Users\prba6\iterations-promotosh\functions")
source("C:/Users/prba6/iterations-promotosh/functions")
source("C:/Users/prba6/iterations-promotosh/functions/data_transformations.r")
source(C:/Users/prba6/iterations-promotosh/functions/data_transformations.r)
source("C:/Users/prba6/iterations-promotosh/functions/data_transformations.r")
source("C:/Users/prba6/iterations-promotosh/functions/data_transformation.r")
stations_metadata_df <- transform_metadata_to_df(stations_metadata)
source("C:/Users/prba6/iterations-promotosh/functions")
source("C:/Users/prba6/iterations-promotosh/functions/data_transformations.r")
source("C:/Users/prba6/iterations-promotosh/functions/data_transformation.r")
source("C:/Users/prba6/iterations-promotosh/functions/data_transformation.r")
source("C:/Users/prba6/iterations-promotosh/functions/data_transformation.r")
